{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906e1608-7282-417a-b25e-59b8c3fbcc2c",
   "metadata": {},
   "source": [
    "This notebook implements two similar approaches using CLIP. </br>\n",
    "\n",
    "I. CLIP-QUERY-TOP-ADJECTIVES </br>\n",
    "1. A QUERY image is fed into a visual transformer of CLIP.\n",
    "2. The distance from this embedding to the embeddings of every adjective in the English language (approximately 28k adjectives) is computed.\n",
    "3. The top N closest adjectives are selected, and their embeddings are obtained (in this example, the embeddings are concatenated and passed through the textual branch of CLIP, but there is also an idea to create separate embeddings for each adjective and average them).\n",
    "4. The top closest DOC images (pre-embedded using an image transformer of CLIP) are retrieved based on the distance to the prompt embedding obtained on the previous step. </br>\n",
    "\n",
    "II. CLIP-QUERY-AND-DOC-TOP-ADJECTIVES </br>\n",
    "1. Steps 1-3 from the previous approach are executed for both QUERY and DOC images. So we have both textual adjective-based QUERY and DOC embeddings now \n",
    "2. After corresponding embeddings are obtained, a distance-based search is carried out using them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcacdf6-1839-49cf-9080-f5ba72d5d4bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "759d5cb8-6351-4549-ac9c-a2e2fb13d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "from random import choices, choice\n",
    "import matplotlib.pyplot as plt\n",
    "from kekit.image import resize_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from io import BytesIO\n",
    "from typing import Any\n",
    "from typing import Tuple, Optional, Union, List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import clip\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e8bbd3d-06b6-4bf9-a0cb-c64f3fde5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22142c7c-cb13-4ad0-bacf-e8c63901a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image paths and adjectives\n",
    "query_img_paths = [] # dresses' images\n",
    "for file in os.listdir('../test_images/query'):\n",
    "    query_fpath = os.path.join('../test_images/query', file)\n",
    "    query_img_paths.append(query_fpath)\n",
    "    \n",
    "doc_img_paths = []  # dildos' images\n",
    "for root, dirs, files in os.walk('../test_images/doc'):\n",
    "    for file in files:\n",
    "        doc_fpath = os.path.join(root, file)\n",
    "        if os.path.isfile(doc_fpath):\n",
    "            doc_img_paths.append(doc_fpath)\n",
    "            \n",
    "with open('../words/adjectives/28K adjectives.txt', 'r') as f:\n",
    "    adjectives = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8286cd4b-6316-4f8d-aaf7-03b0a70f9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample docs if needed:\n",
    "doc_img_paths = random.sample(doc_img_paths, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98d12bc1-fc49-4c42-b91f-263d8a83ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_paths: List[str],\n",
    "        preprocess: Any,\n",
    "        debug: Union[bool, int] = False\n",
    "    ):\n",
    "        self.img_paths = img_paths\n",
    "        self.length = len(self.img_paths)\n",
    "        self.preprocess = preprocess\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int\n",
    "    ) -> Tuple[Optional[str], Optional[torch.tensor]]:\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if image is not None:\n",
    "            image = image.convert(\"RGB\")\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        return img_path, image\n",
    "\n",
    "def images_collate(batch):\n",
    "    batch_paths = []\n",
    "    batch_images = []\n",
    "    for image_path, image in batch:\n",
    "        if image is not None:\n",
    "            batch_paths.append(image_path)\n",
    "            batch_images.append(image)\n",
    "    if len(batch_images) == 0:\n",
    "        return None, None\n",
    "    return batch_paths, torch.stack(batch_images)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        preprocess: Any,\n",
    "        debug: Union[bool, int] = False\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.length = len(self.texts)\n",
    "        self.preprocess = preprocess\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int\n",
    "    ) -> Tuple[Optional[str], Optional[torch.tensor]]:\n",
    "        text = self.texts[idx]\n",
    "        text = self.preprocess(text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "query_img_dataset = ImageDataset(\n",
    "    img_paths=query_img_paths,\n",
    "    preprocess=preprocess\n",
    ")\n",
    "\n",
    "doc_img_dataset = ImageDataset(\n",
    "    img_paths=doc_img_paths,\n",
    "    preprocess=preprocess\n",
    ")\n",
    "\n",
    "def texts_preprocess_dress(text):\n",
    "    text = text.strip().lower()\n",
    "    text = f\"a photo of a {text} dress\"\n",
    "    text = clip.tokenize(text)\n",
    "    return text\n",
    "\n",
    "query_texts_dataset = TextDataset(\n",
    "    texts=adjectives,\n",
    "    preprocess=texts_preprocess_dress,\n",
    ")\n",
    "\n",
    "def texts_preprocess_dildo(text):\n",
    "    text = text.strip().lower()\n",
    "    text = f\"a photo of a {text} dildo\"\n",
    "    text = clip.tokenize(text)\n",
    "    return text\n",
    "\n",
    "doc_texts_dataset = TextDataset(\n",
    "    texts=adjectives,\n",
    "    preprocess=texts_preprocess_dildo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1779acfd-18bf-487a-9fa8-9bd6e083865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.81s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [46:01<00:00,  9.69s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [46:39<00:00,  9.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get features \n",
    "def get_img_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    dloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for labels, images in tqdm(dloader):\n",
    "            features = model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "    return torch.cat(all_features).cpu(), all_labels\n",
    "\n",
    "\n",
    "def get_txt_features(dataset):\n",
    "    all_features = []\n",
    "    dloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=100, shuffle=False\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for texts in tqdm(dloader):\n",
    "            texts = texts.squeeze()\n",
    "            features = model.encode_text(texts.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu()\n",
    "\n",
    "query_img_features, _ = get_img_features(query_img_dataset)\n",
    "doc_img_features, _ = get_img_features(doc_img_dataset)\n",
    "\n",
    "query_text_features = get_txt_features(query_texts_dataset)\n",
    "doc_text_features = get_txt_features(doc_texts_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb70b05-3125-48f7-bc7b-ad91c95b44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features just in case\n",
    "torch.save(query_img_features, '../embeddings/query_img_embeddings_clip.pt')\n",
    "torch.save(doc_img_features, '../embeddings/doc_img_embeddings_clip.pt')\n",
    "torch.save(query_text_features, '../embeddings/query_adjective_embeddings_clip.pt')\n",
    "torch.save(doc_text_features, '../embeddings/doc_adjective_embeddings_clip.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf0d909f-b027-4c4d-936f-2e491802450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some common functions\n",
    "TOPK_WORDS = 10\n",
    "TOPK_DOC_IMGS = 10\n",
    "\n",
    "def get_top_similar(query_embs, doc_embs, topk=100):\n",
    "    similarity = (100.0 * query_embs @ doc_embs.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(topk)\n",
    "    return values, indices\n",
    "\n",
    "\n",
    "def embs_to_text_prompts(embs, text_embs, texts, topk_words=100):\n",
    "    # Takes any clip embeddings (image or text), and for each embedding\n",
    "    # gathers a list of strings from `texts` which are the closest to this input emb\n",
    "    # then concatenates it into a single string essentially getting the text prompt, \n",
    "    # describing the initial embedding\n",
    "    _, text_indices = get_top_similar(embs, text_embs, topk=topk_words)\n",
    "    prompts = []\n",
    "    for single_row_text_indices in text_indices:\n",
    "        prompts.append(' '.join([texts[i] for i in single_row_text_indices]))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f6db395-4c72-42b4-a578-ca83a3b67a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings if needed:\n",
    "# doc_img_embs = torch.load('../embeddings/doc_img_embeddings_clip.pt')\n",
    "# doc_text_embs = torch.load('../embeddings/doc_adjective_embeddings_clip.pt')\n",
    "\n",
    "# query_text_embs = torch.load('../embeddings/query_adjective_embeddings_clip.pt')\n",
    "# query_img_embs = torch.load('../embeddings/query_img_embeddings_clip.pt')\n",
    "\n",
    "query_img_embs = query_img_features\n",
    "doc_img_embs = doc_img_features\n",
    "\n",
    "query_text_embs = query_text_features\n",
    "doc_text_embs = doc_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bfdd80a-9e85-4f73-bae8-a4edf45d2d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 512]),\n",
       " torch.Size([28479, 512]),\n",
       " torch.Size([10, 512]),\n",
       " torch.Size([28479, 512]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize embeddings\n",
    "doc_img_features /= doc_img_embs.norm(dim=-1, keepdim=True)\n",
    "doc_text_embs /= doc_text_embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "query_img_embs /= query_img_embs.norm(dim=-1, keepdim=True)\n",
    "query_text_embs /= query_text_embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "doc_img_embs.shape, doc_text_embs.shape, query_img_embs.shape, query_text_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28ff78b5-5d6a-4a95-9c6f-2e2f25852854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# img to img version\n",
    "def get_prompts_and_doc_indices_by_query_embs(\n",
    "    query_img_embs, \n",
    "    doc_img_embs, \n",
    "    text_embs, \n",
    "    texts, \n",
    "    topk_words=TOPK_WORDS, \n",
    "    topk_doc_imgs=TOPK_DOC_IMGS,\n",
    "    prompt_template=\"a photo of a {} dildo without text no woman\"\n",
    "):\n",
    "    # for each query emb get a string of top closest texts\n",
    "    raw_prompts = embs_to_text_prompts(\n",
    "        query_img_embs, \n",
    "        text_embs, \n",
    "        texts, \n",
    "        topk_words=topk_words\n",
    "    )\n",
    "    \n",
    "    # get the embeddings of the acquired prompts\n",
    "    def prompt_preprocess(text):\n",
    "        text = text.strip().lower()\n",
    "        text = prompt_template.format(text)\n",
    "        text = clip.tokenize(text)\n",
    "        return text\n",
    "\n",
    "    prompts_dataset = TextDataset(\n",
    "        texts=raw_prompts,\n",
    "        preprocess=prompt_preprocess\n",
    "    )\n",
    "\n",
    "    prompts_embs = get_txt_features(prompts_dataset)\n",
    "    \n",
    "    # for each acquired prompt get the closest doc images\n",
    "    _, top_doc_img_indices = get_top_similar(prompts_embs, doc_img_embs, topk=TOPK_DOC_IMGS)\n",
    "    \n",
    "    # return top closest doc images' indices (along with corresponding prompts)\n",
    "    return raw_prompts, top_doc_img_indices\n",
    "\n",
    "\n",
    "prompts, top_doc_img_indices = get_prompts_and_doc_indices_by_query_embs(\n",
    "    query_img_embs, \n",
    "    doc_img_embs, \n",
    "    query_text_embs, \n",
    "    adjectives\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4ae2a-dc24-46b8-b3ee-878451e1c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot img to img version\n",
    "import textwrap\n",
    "\n",
    "fig, axes = plt.subplots(11, 10, figsize=(20, 18))\n",
    "for query_idx, (prompt, doc_indices) in enumerate(zip(prompts, top_doc_img_indices)):\n",
    "    ax = axes[0]\n",
    "    ax[query_idx].imshow(Image.open(query_img_paths[query_idx]))\n",
    "    prompt = '\\n'.join(textwrap.wrap(prompt, width=15))\n",
    "    ax[query_idx].set_title(prompt)\n",
    "    ax[query_idx].set_xticks([])\n",
    "    ax[query_idx].set_yticks([])\n",
    "    for axi, doc_idx in enumerate(doc_indices):\n",
    "        axes[axi + 1][query_idx].imshow(Image.open(doc_img_paths[doc_idx]))\n",
    "        axes[axi + 1][query_idx].set_xticks([])\n",
    "        axes[axi + 1][query_idx].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86bad64c-23ab-48ec-b315-fcf8eb3c81a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# txt to txt version\n",
    "def get_similar_by_text_embs(\n",
    "    query_img_embs,\n",
    "    doc_img_embs,\n",
    "    query_words_embs,  # for a single adjective\n",
    "    doc_words_embs,  # for a single adjective\n",
    "    texts,  # unified for both query/doc \n",
    "    topk_words=TOPK_WORDS, \n",
    "    topk_doc_imgs=TOPK_DOC_IMGS,\n",
    "    query_prompt_template=\"a photo of a {} dress\",\n",
    "    doc_prompt_template=\"a photo of a {} dildo\"\n",
    "):\n",
    "    # for each query emb get a string of top closest texts\n",
    "    raw_query_prompts = embs_to_text_prompts(\n",
    "        query_img_embs, \n",
    "        query_words_embs, \n",
    "        texts, \n",
    "        topk_words=topk_words\n",
    "    )\n",
    "\n",
    "    # get the embeddings of the acquired query prompts\n",
    "    def query_prompt_preprocess(text):\n",
    "        text = text.strip().lower()\n",
    "        text = query_prompt_template.format(text)\n",
    "        text = clip.tokenize(text)\n",
    "        return text\n",
    "\n",
    "    query_prompts_dataset = TextDataset(\n",
    "        texts=raw_query_prompts,\n",
    "        preprocess=query_prompt_preprocess\n",
    "    )\n",
    "\n",
    "    query_prompts_embs = get_txt_features(query_prompts_dataset)\n",
    "    \n",
    "    \n",
    "    # for each doc emb get a string of top closest texts\n",
    "    raw_doc_prompts = embs_to_text_prompts(\n",
    "        doc_img_embs, \n",
    "        doc_words_embs, \n",
    "        texts, \n",
    "        topk_words=topk_words\n",
    "    )\n",
    "\n",
    "    # get the embeddings of the acquired doc prompts\n",
    "    def doc_prompt_preprocess(text):\n",
    "        text = text.strip().lower()\n",
    "        text = doc_prompt_template.format(text)\n",
    "        text = clip.tokenize(text)\n",
    "        return text\n",
    "\n",
    "    doc_prompts_dataset = TextDataset(\n",
    "        texts=raw_doc_prompts,\n",
    "        preprocess=doc_prompt_preprocess\n",
    "    )\n",
    "\n",
    "    doc_prompts_embs = get_txt_features(doc_prompts_dataset)\n",
    "    \n",
    "    \n",
    "    # for each query prompt emb find closest doc prompt emb \n",
    "    # (similarity scores and indices)\n",
    "    _, top_doc_img_indices = get_top_similar(\n",
    "        query_prompts_embs,\n",
    "        doc_prompts_embs,\n",
    "        topk=TOPK_DOC_IMGS\n",
    "    )\n",
    "    \n",
    "    return raw_query_prompts, top_doc_img_indices, raw_doc_prompts\n",
    "\n",
    "\n",
    "raw_query_prompts, top_doc_img_indices, raw_doc_prompts = get_similar_by_text_embs(\n",
    "    query_img_embs,\n",
    "    doc_img_embs,\n",
    "    query_words_embs=query_text_embs,  # for a single adjective\n",
    "    doc_words_embs=doc_text_embs,  # for a single adjective\n",
    "    texts=adjectives,  # unified for both query/doc \n",
    ")\n",
    "\n",
    "doc_prompts_selected = []\n",
    "for top_doc_idx_row in top_doc_img_indices:\n",
    "    doc_prompts_selected.append([raw_doc_prompts[x] for x in top_doc_idx_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5b82f-57f1-4ca5-8092-872263d9cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot txt to txt version\n",
    "import textwrap\n",
    "\n",
    "fig, axes = plt.subplots(11, 10, figsize=(20, 40))\n",
    "for query_idx, (prompt, doc_indices) in enumerate(zip(raw_query_prompts, top_doc_img_indices)):\n",
    "    ax = axes[0]\n",
    "    ax[query_idx].imshow(Image.open(query_img_paths[query_idx]))\n",
    "    prompt = '\\n'.join(textwrap.wrap(prompt, width=15))\n",
    "    ax[query_idx].set_title(prompt)\n",
    "    ax[query_idx].set_xticks([])\n",
    "    ax[query_idx].set_yticks([])\n",
    "    doc_prompts = doc_prompts_selected[query_idx]\n",
    "    for axi, (doc_idx, doc_prompt) in enumerate(zip(doc_indices, doc_prompts)):\n",
    "        wrapped_prompt = '\\n'.join(textwrap.wrap(doc_prompt, width=15))\n",
    "        axes[axi + 1][query_idx].imshow(Image.open(doc_img_paths[doc_idx]))\n",
    "        axes[axi + 1][query_idx].set_title(wrapped_prompt)\n",
    "        axes[axi + 1][query_idx].set_xticks([])\n",
    "        axes[axi + 1][query_idx].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee80509-b2bc-48bb-a421-8eea5f8977c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topd_models 3.7",
   "language": "python",
   "name": "topd_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
